{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "5e8eeb30", "cell_type": "markdown", "source": "# \ud83d\udcca Model Evaluation on instruct_dataset.jsonl\n\nThis notebook evaluates the performance of a fine-tuned model using the original training dataset `instruct_dataset.jsonl`.\n\nThe comparison is made between the model's predictions and the ground truth JSON outputs.", "metadata": {}}, {"id": "dcecdfb2", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "!pip install transformers datasets", "outputs": []}, {"id": "cda09ef6", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load your fine-tuned model (adjust path if needed)\nmodel_path = \"./checkpoints/final\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "outputs": []}, {"id": "05777b1b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import json\n\n# Load instruct_dataset.jsonl\nwith open(\"./dataset/instruct_dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n    dataset = [json.loads(line) for line in f]\n\nprint(f\"Loaded {len(dataset)} samples\")\n", "outputs": []}, {"id": "276756fe", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from tqdm import tqdm\n\ncorrect = 0\ntotal = 0\nfailed = 0\n\nfor item in tqdm(dataset):\n    prompt = f\"{item['instruction']}\n\n{item['input']}\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=128)\n\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    try:\n        predicted_json = json.loads(generated)\n        if predicted_json == item[\"output\"]:\n            correct += 1\n    except:\n        failed += 1  # JSON \ud30c\uc2f1 \uc2e4\ud328\n\n    total += 1\n\nprint(f\"Total: {total}\")\nprint(f\"Correct JSON Matches: {correct}\")\nprint(f\"Parse Failures: {failed}\")\nprint(f\"Accuracy: {correct / total:.2%}\")\n", "outputs": []}]}